{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ライブラリ読み込み"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/keimy/git/humanized-models-for-board-games/igo\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/keimy/git/humanized-models-for-board-games/igo'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd; pd.set_option('display.max_columns', None)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# utility\n",
    "import time\n",
    "import copy as cp\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from sys import stderr\n",
    "\n",
    "# 乱数\n",
    "rng = np.random.RandomState(1234)\n",
    "random_state = 42\n",
    "\n",
    "# pytorch\n",
    "# conda install pytorch torchvision -c pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.autograd as autograd\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "# 囲碁ライブラリ\n",
    "from sgfmill import sgf\n",
    "from sgfmill import boards\n",
    "\n",
    "# ルートに移動\n",
    "%cd /Users/keimy/git/humanized-models-for-board-games/igo\n",
    "%pwd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55191, 2, 19, 19)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .npy ファイルからデータをロード\n",
    "move_tensors = np.load('./preprocess/tensors_at_move.npy')\n",
    "\n",
    "move_tensors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.board import visualize_matrix\n",
    "\n",
    "black_matrix, white_matrix = move_tensors[200]\n",
    "visualize_matrix(black_matrix, white_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# トレーニングセットと検証セットに分割\n",
    "train_size = int(0.8 * len(move_tensors))\n",
    "valid_size = len(move_tensors) - train_size\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(move_tensors, [train_size, valid_size])\n",
    "\n",
    "# データローダーの設定\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 2, 19, 19])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "345"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x in train_loader:\n",
    "    print(x.shape)\n",
    "    # print(x)\n",
    "    break\n",
    "\n",
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE-linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model.VAE_linear import VAE_linear\n",
    "z_dim = 10\n",
    "assert z_dim >= 2\n",
    "input_dim =  2 * 19 * 19\n",
    "n_epochs = 3\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "\n",
    "model = VAE_linear(z_dim = z_dim, input_dim = input_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    losses = []\n",
    "    KL_losses = []\n",
    "    reconstruction_losses = []\n",
    "\n",
    "    model.train()\n",
    "    for x in train_loader:\n",
    "        x = x.float().to(device)  # データ型をfloatに変換してからデバイスに送る\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        # KL_loss, reconstruction_lossの各項の計算\n",
    "        KL_loss, reconstruction_loss = model.loss(x)\n",
    "\n",
    "        # エビデンス下界の最大化のためマイナス付きの各項の値を最小化するようにパラメータを更新\n",
    "        loss = KL_loss + reconstruction_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "        KL_losses.append(KL_loss.cpu().detach().numpy())\n",
    "        reconstruction_losses.append(reconstruction_loss.cpu().detach().numpy())\n",
    "\n",
    "    losses_val = []\n",
    "    model.eval()\n",
    "    for x in valid_loader:\n",
    "        x = x.float().to(device)  # ここでも同様にデータ型をfloatに変換\n",
    "\n",
    "        KL_loss, reconstruction_loss = model.loss(x)\n",
    "\n",
    "        loss = KL_loss + reconstruction_loss\n",
    "\n",
    "        losses_val.append(loss.cpu().detach().numpy())\n",
    "\n",
    "    print('EPOCH: %d    Train Lower Bound: %lf (KL_loss: %lf. reconstruction_loss: %lf)    Valid Lower Bound: %lf' %\n",
    "          (epoch+1, np.average(losses), np.average(KL_losses), np.average(reconstruction_losses), np.average(losses_val)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Reconstruction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "任意のxをもとに可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xはバッチで投げればy,zもバッチで返ってくる\n",
    "\n",
    "\n",
    "xを単体で投げた場合はバッチサイズ=1と解釈されて\n",
    "\n",
    "y,zもバッチサイズ=1の次元になるのでスクイーズが必要"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Xをバッチで投げる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "for x in valid_loader:\n",
    "    print(x.shape, x.dtype)\n",
    "    # xをfloatに変換\n",
    "    x = x.float().to(device)\n",
    "\n",
    "    y, z = model(x)\n",
    "    y = y.cpu().detach().numpy()\n",
    "\n",
    "    print(y.shape, z.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "xを単体で投げる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "i = 40\n",
    "x = valid_dataset[i]\n",
    "x = torch.from_numpy(x).float()\n",
    "x = x.to(device)\n",
    "\n",
    "y, z = model(x)\n",
    "y = y.cpu().detach().numpy()\n",
    "\n",
    "\n",
    "x.shape, y.shape, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "i = 40\n",
    "x = valid_dataset[i]\n",
    "x = torch.from_numpy(x).float()\n",
    "x = x.to(device)\n",
    "\n",
    "y, z = model(x)\n",
    "y = y.cpu().detach().numpy()\n",
    "\n",
    "# yの閾値処理\n",
    "th = 0.22\n",
    "y[y >= th] = 1\n",
    "y[y < th] = 0\n",
    "\n",
    "print(z)\n",
    "\n",
    "print(\"reconstructed move\")\n",
    "black_matrix, white_matrix = y.squeeze()\n",
    "visualize_matrix(black_matrix, white_matrix)\n",
    "\n",
    "print(\"original move\")\n",
    "black_matrix, white_matrix = x.squeeze()\n",
    "visualize_matrix(black_matrix, white_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zのランダムサンプリング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn([1, z_dim]).to(device)\n",
    "\n",
    "model.eval()\n",
    "y = model._decoder(z)\n",
    "y = y.cpu().detach().numpy()\n",
    "\n",
    "# yの閾値処理\n",
    "th = 0.2\n",
    "y[y >= th] = 1\n",
    "y[y < th] = 0\n",
    "\n",
    "print(z)\n",
    "black_matrix, white_matrix = y.squeeze()\n",
    "visualize_matrix(black_matrix, white_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VAE-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'src.model.VAE_CNN' from '/Users/keimy/git/humanized-models-for-board-games/igo/src/model/VAE_CNN.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "from src.model import VAE_CNN\n",
    "\n",
    "# VAE_CNN モジュールを再読み込み\n",
    "importlib.reload(VAE_CNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1    Train Lower Bound: 26937.388672 (KL_loss: 552.334961. reconstruction_loss: 26385.052734)    Valid Lower Bound: 25028.041016\n",
      "EPOCH: 2    Train Lower Bound: 24771.683594 (KL_loss: 992.663940. reconstruction_loss: 23779.019531)    Valid Lower Bound: 24130.072266\n",
      "EPOCH: 3    Train Lower Bound: 23905.923828 (KL_loss: 1255.472412. reconstruction_loss: 22650.449219)    Valid Lower Bound: 23353.917969\n",
      "EPOCH: 4    Train Lower Bound: 23024.365234 (KL_loss: 1631.979858. reconstruction_loss: 21392.384766)    Valid Lower Bound: 22219.892578\n",
      "EPOCH: 5    Train Lower Bound: 21098.642578 (KL_loss: 2312.492676. reconstruction_loss: 18786.152344)    Valid Lower Bound: 19525.425781\n"
     ]
    }
   ],
   "source": [
    "from src.model.VAE_CNN import VAE_CNN\n",
    "z_dim = 64\n",
    "assert z_dim >= 2\n",
    "input_dim =  2 * 19 * 19\n",
    "n_epochs = 5\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "\n",
    "model = VAE_CNN(z_dim = z_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    losses = []\n",
    "    KL_losses = []\n",
    "    reconstruction_losses = []\n",
    "\n",
    "    model.train()\n",
    "    for x in train_loader:\n",
    "        x = x.float().to(device)  # データ型をfloatに変換してからデバイスに送る\n",
    "\n",
    "        model.zero_grad()\n",
    "\n",
    "        # KL_loss, reconstruction_lossの各項の計算\n",
    "        KL_loss, reconstruction_loss = model.loss(x)\n",
    "\n",
    "        # エビデンス下界の最大化のためマイナス付きの各項の値を最小化するようにパラメータを更新\n",
    "        loss = KL_loss + reconstruction_loss\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.cpu().detach().numpy())\n",
    "        KL_losses.append(KL_loss.cpu().detach().numpy())\n",
    "        reconstruction_losses.append(reconstruction_loss.cpu().detach().numpy())\n",
    "\n",
    "    losses_val = []\n",
    "    model.eval()\n",
    "    for x in valid_loader:\n",
    "        x = x.float().to(device)  # ここでも同様にデータ型をfloatに変換\n",
    "\n",
    "        KL_loss, reconstruction_loss = model.loss(x)\n",
    "\n",
    "        loss = KL_loss + reconstruction_loss\n",
    "\n",
    "        losses_val.append(loss.cpu().detach().numpy())\n",
    "\n",
    "    print('EPOCH: %d    Train Lower Bound: %lf (KL_loss: %lf. reconstruction_loss: %lf)    Valid Lower Bound: %lf' %\n",
    "          (epoch+1, np.average(losses), np.average(KL_losses), np.average(reconstruction_losses), np.average(losses_val)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "linear(): input and weight.T shapes cannot be multiplied (32x361 and 11552x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/keimy/git/humanized-models-for-board-games/igo/notebooks/[ks231213]VAE.ipynb セル 24\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/keimy/git/humanized-models-for-board-games/igo/notebooks/%5Bks231213%5DVAE.ipynb#X22sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(x)\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/keimy/git/humanized-models-for-board-games/igo/notebooks/%5Bks231213%5DVAE.ipynb#X22sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/keimy/git/humanized-models-for-board-games/igo/notebooks/%5Bks231213%5DVAE.ipynb#X22sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m y, z_mu, z_var \u001b[39m=\u001b[39m model(x)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/keimy/git/humanized-models-for-board-games/igo/notebooks/%5Bks231213%5DVAE.ipynb#X22sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# y = y.cpu().detach().numpy()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/keimy/git/humanized-models-for-board-games/igo/notebooks/%5Bks231213%5DVAE.ipynb#X22sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/keimy/git/humanized-models-for-board-games/igo/notebooks/%5Bks231213%5DVAE.ipynb#X22sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# # yの閾値処理\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/keimy/git/humanized-models-for-board-games/igo/notebooks/%5Bks231213%5DVAE.ipynb#X22sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39m# black_matrix, white_matrix = x.squeeze()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/keimy/git/humanized-models-for-board-games/igo/notebooks/%5Bks231213%5DVAE.ipynb#X22sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# visualize_matrix(black_matrix, white_matrix)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/world_model/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/world_model/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/git/humanized-models-for-board-games/igo/src/model/VAE_CNN.py:83\u001b[0m, in \u001b[0;36mVAE_CNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 83\u001b[0m     mean, log_var \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x)\n\u001b[1;32m     84\u001b[0m     z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreparameterize(mean, log_var)\n\u001b[1;32m     85\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(z), mean, log_var\n",
      "File \u001b[0;32m~/anaconda3/envs/world_model/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/world_model/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/git/humanized-models-for-board-games/igo/src/model/VAE_CNN.py:45\u001b[0m, in \u001b[0;36mCNNEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(x))\n\u001b[1;32m     44\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> 45\u001b[0m x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfc1(x))\n\u001b[1;32m     46\u001b[0m mean \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_mean(x)\n\u001b[1;32m     47\u001b[0m log_var \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfc_var(x)\n",
      "File \u001b[0;32m~/anaconda3/envs/world_model/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/world_model/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/world_model/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: linear(): input and weight.T shapes cannot be multiplied (32x361 and 11552x128)"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "i = 40\n",
    "x = valid_dataset[i]\n",
    "x = torch.from_numpy(x).float()\n",
    "x = x.to(device)\n",
    "\n",
    "y, z_mu, z_var = model(x)\n",
    "# y = y.cpu().detach().numpy()\n",
    "\n",
    "# # yの閾値処理\n",
    "# th = 0.22\n",
    "# y[y >= th] = 1\n",
    "# y[y < th] = 0\n",
    "\n",
    "# print(z_mu, z_var)\n",
    "\n",
    "# print(\"reconstructed move\")\n",
    "# black_matrix, white_matrix = y.squeeze()\n",
    "# visualize_matrix(black_matrix, white_matrix)\n",
    "\n",
    "# print(\"original move\")\n",
    "# black_matrix, white_matrix = x.squeeze()\n",
    "# visualize_matrix(black_matrix, white_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
